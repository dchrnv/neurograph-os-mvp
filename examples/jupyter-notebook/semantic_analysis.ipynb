{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroGraph Semantic Analysis with Jupyter\n",
    "\n",
    "This notebook demonstrates using NeuroGraph Python client for semantic analysis and document clustering.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install neurograph-python matplotlib scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "from neurograph import NeuroGraphClient\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to NeuroGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = NeuroGraphClient(\n",
    "    base_url=\"http://localhost:8000\",\n",
    "    username=\"developer\",\n",
    "    password=\"developer123\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "health = client.health.check()\n",
    "print(f\"Connected to NeuroGraph {health.version}\")\n",
    "print(f\"Status: {health.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents from different categories\n",
    "documents = [\n",
    "    # Technology\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning algorithms can recognize patterns\",\n",
    "    \"Deep neural networks are used in AI\",\n",
    "    \"JavaScript is popular for web development\",\n",
    "    \"Cloud computing provides scalable infrastructure\",\n",
    "    \n",
    "    # Science\n",
    "    \"DNA contains genetic information\",\n",
    "    \"Photosynthesis converts light into energy\",\n",
    "    \"Evolution explains biological diversity\",\n",
    "    \"Atoms are the basic units of matter\",\n",
    "    \"The theory of relativity revolutionized physics\",\n",
    "    \n",
    "    # Business\n",
    "    \"Marketing strategies increase brand awareness\",\n",
    "    \"Financial planning is essential for businesses\",\n",
    "    \"Customer service improves client satisfaction\",\n",
    "    \"Supply chain management optimizes operations\",\n",
    "    \"Data analytics drives business decisions\",\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    'tech', 'tech', 'tech', 'tech', 'tech',\n",
    "    'science', 'science', 'science', 'science', 'science',\n",
    "    'business', 'business', 'business', 'business', 'business'\n",
    "]\n",
    "\n",
    "# Create tokens\n",
    "print(\"Creating tokens...\")\n",
    "tokens = []\n",
    "for text, category in zip(documents, categories):\n",
    "    token = client.tokens.create(\n",
    "        text=text,\n",
    "        metadata={\"category\": category, \"source\": \"jupyter_notebook\"}\n",
    "    )\n",
    "    tokens.append(token)\n",
    "    print(f\"✓ Created token {token.id}: {text[:50]}...\")\n",
    "\n",
    "print(f\"\\nCreated {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings and metadata\n",
    "embeddings = np.array([token.embedding for token in tokens])\n",
    "texts = [token.text for token in tokens]\n",
    "labels = [token.metadata['category'] for token in tokens]\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with t-SNE\n",
    "print(\"Running t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    'x': embeddings_2d[:, 0],\n",
    "    'y': embeddings_2d[:, 1],\n",
    "    'text': texts,\n",
    "    'category': labels\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = {'tech': 'blue', 'science': 'green', 'business': 'red'}\n",
    "\n",
    "for category in df['category'].unique():\n",
    "    mask = df['category'] == category\n",
    "    plt.scatter(\n",
    "        df[mask]['x'],\n",
    "        df[mask]['y'],\n",
    "        c=colors[category],\n",
    "        label=category,\n",
    "        s=100,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "plt.title('Document Embeddings Visualization (t-SNE)', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDocuments by category:\")\n",
    "print(df.groupby('category').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for similar documents\n",
    "query = \"artificial intelligence and neural networks\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Create query token\n",
    "query_token = client.tokens.create(text=query)\n",
    "\n",
    "# Search\n",
    "results = client.tokens.query(\n",
    "    query_vector=query_token.embedding,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Top 5 similar documents:\\n\")\n",
    "results_data = []\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Similarity: {result.similarity:.4f}\")\n",
    "    print(f\"   Text: {result.token.text}\")\n",
    "    print(f\"   Category: {result.token.metadata.get('category')}\\n\")\n",
    "    \n",
    "    results_data.append({\n",
    "        'rank': i,\n",
    "        'similarity': result.similarity,\n",
    "        'text': result.token.text,\n",
    "        'category': result.token.metadata.get('category')\n",
    "    })\n",
    "\n",
    "# Cleanup query token\n",
    "client.tokens.delete(query_token.id)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Add clusters to DataFrame\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# Plot with cluster assignments\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    df['x'],\n",
    "    df['y'],\n",
    "    c=df['cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Plot cluster centers in 2D space\n",
    "centers_2d = tsne.fit_transform(kmeans.cluster_centers_)\n",
    "plt.scatter(\n",
    "    centers_2d[:, 0],\n",
    "    centers_2d[:, 1],\n",
    "    c='red',\n",
    "    marker='X',\n",
    "    s=200,\n",
    "    label='Cluster Centers',\n",
    "    edgecolors='black'\n",
    ")\n",
    "\n",
    "plt.title(f'K-Means Clustering (k={n_clusters})', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nCluster composition:\")\n",
    "cluster_composition = df.groupby(['cluster', 'category']).size().unstack(fill_value=0)\n",
    "print(cluster_composition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix (cosine similarity)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=[f\"{i}: {t[:30]}...\" for i, t in enumerate(texts)],\n",
    "    yticklabels=[f\"{i}: {t[:30]}...\" for i, t in enumerate(texts)],\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "plt.title('Document Similarity Matrix', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most similar pairs\n",
    "print(\"\\nTop 5 most similar document pairs:\")\n",
    "similarity_pairs = []\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        similarity_pairs.append({\n",
    "            'doc1': texts[i][:40],\n",
    "            'doc2': texts[j][:40],\n",
    "            'similarity': similarity_matrix[i, j]\n",
    "        })\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_pairs)\n",
    "similarity_df = similarity_df.sort_values('similarity', ascending=False)\n",
    "print(similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete created tokens (optional)\n",
    "print(\"Cleaning up tokens...\")\n",
    "for token in tokens:\n",
    "    client.tokens.delete(token.id)\n",
    "    print(f\"✓ Deleted token {token.id}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")\n",
    "\n",
    "# Close client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
