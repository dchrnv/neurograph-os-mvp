"""
Example usage of NeuroGraph OS persistence layer.
Demonstrates database operations with PostgreSQL and Redis.
"""

import asyncio
from uuid import uuid4
from datetime import datetime
import time

from src.infrastructure.config import ConfigLoader
from src.infrastructure.persistence.database import DatabaseFactory
from src.infrastructure.persistence.repositories import RepositoryFactory
from src.infrastructure.persistence.models import TokenModel, ConnectionModel


async def main():
    """Main example function."""
    
    # 1. Load configuration
    print("üìã Loading configuration...")
    config_loader = ConfigLoader()
    db_config = config_loader.load('infrastructure/database.yaml')
    
    # 2. Initialize database managers
    print("üóÑÔ∏è  Initializing database connections...")
    db_manager = await DatabaseFactory.create_database_manager(db_config['database'])
    redis_manager = DatabaseFactory.create_redis_manager(db_config['database'])
    
    # 3. Check health
    print("üè• Checking database health...")
    pg_health = await db_manager.health_check()
    redis_health = redis_manager.health_check()
    print(f"   PostgreSQL: {'‚úÖ' if pg_health else '‚ùå'}")
    print(f"   Redis: {'‚úÖ' if redis_health else '‚ùå'}")
    
    if not (pg_health and redis_health):
        print("‚ùå Database health check failed!")
        return
    
    # 4. Create tables (development only!)
    print("üèóÔ∏è  Creating database tables...")
    await db_manager.create_tables()
    
    # 5. Work with repositories
    async with db_manager.session() as session:
        
        # Create repositories
        token_repo = RepositoryFactory.create_token_repository(
            session, 
            redis_manager,
            db_config['repositories']['token']
        )
        graph_repo = RepositoryFactory.create_graph_repository(
            session,
            redis_manager,
            db_config['repositories']['graph']
        )
        
        print("\nüìù Creating tokens...")
        
        # Create sample tokens
        token1 = TokenModel(
            id=uuid4(),
            binary_data=b'\x00' * 64,
            coord_x=[1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125],
            coord_y=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            coord_z=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            flags=0b1010,
            weight=1.0,
            timestamp=int(time.time() * 1000),
            token_type='test',
            metadata={'description': 'Test token 1'}
        )
        
        token2 = TokenModel(
            id=uuid4(),
            binary_data=b'\x00' * 64,
            coord_x=[2.0, 1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625],
            coord_y=[1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125],
            coord_z=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            flags=0b1100,
            weight=2.0,
            timestamp=int(time.time() * 1000),
            token_type='test',
            metadata={'description': 'Test token 2'}
        )
        
        # Save tokens
        token1 = await token_repo.create(token1)
        token2 = await token_repo.create(token2)
        print(f"   ‚úÖ Created token 1: {token1.id}")
        print(f"   ‚úÖ Created token 2: {token2.id}")
        
        # Create connection
        print("\nüîó Creating graph connection...")
        connection = await graph_repo.create_connection(
            source_id=token1.id,
            target_id=token2.id,
            connection_type='spatial',
            weight=0.8,
            metadata={'distance': 1.414}
        )
        print(f"   ‚úÖ Created connection: {connection.id}")
        
        # Retrieve token (will use cache on second call)
        print("\nüîç Retrieving token...")
        retrieved = await token_repo.get_by_id(token1.id, use_cache=True)
        print(f"   ‚úÖ Retrieved: {retrieved.id} (type: {retrieved.token_type})")
        
        # Get neighbors
        print("\nüë• Finding neighbors...")
        neighbors = await graph_repo.get_neighbors(token1.id, direction='outgoing')
        print(f"   ‚úÖ Found {len(neighbors)} neighbor(s)")
        for conn in neighbors:
            print(f"      ‚Üí {conn.target_id} (type: {conn.connection_type}, weight: {conn.weight})")
        
        # Spatial query
        print("\nüó∫Ô∏è  Spatial query...")
        nearby_tokens = await token_repo.find_in_region(
            min_coords=(0.0, 0.0, 0.0),
            max_coords=(3.0, 3.0, 3.0),
            level=0
        )
        print(f"   ‚úÖ Found {len(nearby_tokens)} token(s) in region")
        
        # Get statistics
        print("\nüìä Graph statistics...")
        degree = await graph_repo.get_degree(token1.id)
        print(f"   In-degree: {degree['in_degree']}")
        print(f"   Out-degree: {degree['out_degree']}")
        print(f"   Total degree: {degree['total_degree']}")
        
        # Count total tokens
        total_tokens = await token_repo.count()
        print(f"\nüìà Total tokens in database: {total_tokens}")
    
    # 6. Test Redis caching
    print("\nüíæ Testing Redis cache...")
    cache_key = redis_manager.build_key('token_by_id', token_id=str(token1.id))
    cached_value = redis_manager.get_client().get(cache_key)
    print(f"   Cache key: {cache_key}")
    print(f"   Cached: {'‚úÖ Yes' if cached_value else '‚ùå No'}")
    
    # 7. Cleanup
    print("\nüßπ Cleaning up...")
    await DatabaseFactory.close_all()
    print("   ‚úÖ Connections closed")
    
    print("\n‚ú® Example completed successfully!")


async def bulk_insert_example():
    """Example of bulk insert operations."""
    
    print("\n" + "="*60)
    print("BULK INSERT EXAMPLE")
    print("="*60)
    
    # Load config
    config_loader = ConfigLoader()
    db_config = config_loader.load('infrastructure/database.yaml')
    
    # Initialize
    db_manager = await DatabaseFactory.create_database_manager(db_config['database'])
    
    async with db_manager.session() as session:
        token_repo = RepositoryFactory.create_token_repository(
            session, 
            config=db_config['repositories']['token']
        )
        
        print(f"üì¶ Creating 1000 tokens in bulk...")
        start_time = time.time()
        
        # Generate tokens
        tokens = []
        for i in range(1000):
            token = TokenModel(
                id=uuid4(),
                binary_data=b'\x00' * 64,
                coord_x=[float(i)] * 8,
                coord_y=[0.0] * 8,
                coord_z=[0.0] * 8,
                flags=i % 16,
                weight=1.0,
                timestamp=int(time.time() * 1000),
                token_type='bulk_test',
                metadata={'batch': 'example', 'index': i}
            )
            tokens.append(token)
        
        # Bulk insert
        await token_repo.bulk_create(tokens)
        
        elapsed = time.time() - start_time
        print(f"   ‚úÖ Inserted 1000 tokens in {elapsed:.2f}s")
        print(f"   ‚ö° Rate: {1000/elapsed:.0f} tokens/sec")
    
    await DatabaseFactory.close_all()


async def experience_example():
    """Example of working with Experience events."""
    
    print("\n" + "="*60)
    print("EXPERIENCE EVENTS EXAMPLE")
    print("="*60)
    
    # Load config
    config_loader = ConfigLoader()
    db_config = config_loader.load('infrastructure/database.yaml')
    
    # Initialize
    db_manager = await DatabaseFactory.create_database_manager(db_config['database'])
    
    async with db_manager.session() as session:
        exp_repo = RepositoryFactory.create_experience_repository(
            session,
            config=db_config['repositories']['experience']
        )
        
        print("üìù Creating experience events...")
        
        # Create events
        for i in range(5):
            event = await exp_repo.create_event(
                event_type='test_action',
                timestamp=int(time.time() * 1000) + i,
                state_before={'value': i},
                state_after={'value': i + 1},
                action={'type': 'increment'},
                reward=1.0,
                metadata={'step': i}
            )
            print(f"   ‚úÖ Event {i+1}: {event.id}")
        
        # Query recent events
        print("\nüîç Querying recent events...")
        recent = await exp_repo.get_recent_events(count=3)
        print(f"   Found {len(recent)} recent event(s)")
        for event in recent:
            print(f"      ‚Üí Type: {event.event_type}, Reward: {event.reward}")
    
    await DatabaseFactory.close_all()


async def migration_example():
    """Example of running database migrations."""
    
    print("\n" + "="*60)
    print("DATABASE MIGRATION EXAMPLE")
    print("="*60)
    
    print("""
To run migrations:

1. Generate migration:
   alembic revision --autogenerate -m "Initial schema"

2. Apply migration:
   alembic upgrade head

3. Rollback migration:
   alembic downgrade -1

4. Show current version:
   alembic current

5. Show migration history:
   alembic history
    """)


def docker_setup_guide():
    """Print Docker setup instructions."""
    
    print("\n" + "="*60)
    print("DOCKER SETUP GUIDE")
    print("="*60)
    
    print("""
1. Start PostgreSQL and Redis:
   docker-compose -f docker-compose.db.yml up -d

2. Check services status:
   docker-compose -f docker-compose.db.yml ps

3. View logs:
   docker-compose -f docker-compose.db.yml logs -f postgres
   docker-compose -f docker-compose.db.yml logs -f redis

4. Stop services:
   docker-compose -f docker-compose.db.yml down

5. Start with management tools (PgAdmin, Redis Commander):
   docker-compose -f docker-compose.db.yml --profile tools up -d

6. Access management UIs:
   - PgAdmin: http://localhost:5050 (admin@neurograph.local / admin)
   - Redis Commander: http://localhost:8081

7. Clean up (removes volumes):
   docker-compose -f docker-compose.db.yml down -v
    """)


def environment_setup_guide():
    """Print environment setup instructions."""
    
    print("\n" + "="*60)
    print("ENVIRONMENT SETUP")
    print("="*60)
    
    print("""
Create .env file in project root:

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=neurograph
POSTGRES_USER=neurograph_user
POSTGRES_PASSWORD=your_secure_password_here

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Application
LOG_LEVEL=INFO
ENVIRONMENT=development
    """)


if __name__ == "__main__":
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë        NeuroGraph OS - Persistence Layer Examples        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Print guides
    docker_setup_guide()
    environment_setup_guide()
    
    # Run examples
    print("\n" + "="*60)
    print("RUNNING EXAMPLES")
    print("n"*60)
    
    try:
        # Main CRUD example
        asyncio.run(main())
        
        # Bulk insert example
        asyncio.run(bulk_insert_example())
        
        # Experience events example
        asyncio.run(experience_example())
        
        # Migration info
        asyncio.run(migration_example())
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("Examples completed!")
    print("="*60)