// NeuroGraph - Высокопроизводительная система пространственных вычислений на основе токенов.
// Copyright (C) 2024-2025 Chernov Denys

// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.

// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.

// You should have received a copy of the GNU Affero General Public License
// along with this program. If not, see <https://www.gnu.org/licenses/>.

//! Policy Interface and Implementations
//!
//! **Version:** 3.0.0
//!
//! Policies map states to actions. This module defines the Policy trait
//! and provides basic implementations (Linear, Neural, TreeBased, etc.)

use crate::archive::ExperienceToken;
use std::fmt::Debug;

// ============================================================================
// Core Policy Trait
// ============================================================================

/// Policy trait - maps states to actions
///
/// All policies must implement this trait to be compatible with ADNA.
/// Policies can be linear, neural, tree-based, or hybrid combinations.
pub trait Policy: Debug + Send + Sync {
    /// Map state to action
    ///
    /// # Arguments
    /// * `state` - 8D state vector (L1-L8 semantic space)
    ///
    /// # Returns
    /// 8D action vector
    fn map_state(&self, state: &[f32; 8]) -> [f32; 8];

    /// Calculate gradient from experience
    ///
    /// Analyzes experience token to determine how policy should change.
    ///
    /// # Arguments
    /// * `experience` - Experience token containing state-action-reward
    ///
    /// # Returns
    /// Gradient update for this policy
    fn get_gradient(&self, experience: &ExperienceToken) -> Gradient;

    /// Apply gradient update to policy
    ///
    /// Modifies policy parameters based on gradient.
    ///
    /// # Arguments
    /// * `gradient` - Gradient to apply
    /// * `learning_rate` - Step size for update
    ///
    /// # Returns
    /// Result indicating success or error
    fn apply_gradient(&mut self, gradient: &Gradient, learning_rate: f32) -> Result<(), PolicyError>;

    /// Validate that action is within bounds
    ///
    /// # Arguments
    /// * `action` - Action to validate
    /// * `bounds` - Min/max bounds for action space
    ///
    /// # Returns
    /// true if action is valid
    fn validate_action(&self, action: &[f32; 8], bounds: &[f32; 4]) -> bool {
        for &val in action.iter() {
            if val < bounds[0] || val > bounds[1] {
                return false;
            }
        }
        true
    }

    /// Serialize policy to bytes
    fn serialize(&self) -> Vec<u8>;

    /// Deserialize policy from bytes
    fn deserialize(data: &[u8]) -> Result<Self, PolicyError>
    where
        Self: Sized;

    /// Get policy size in bytes
    fn size(&self) -> usize {
        self.serialize().len()
    }
}

// ============================================================================
// Gradient Structure
// ============================================================================

/// Gradient represents proposed changes to policy
#[derive(Debug, Clone)]
pub struct Gradient {
    /// Policy-specific delta (e.g., weight changes)
    pub delta: Vec<f32>,

    /// Confidence in this update (0.0 - 1.0)
    pub confidence: f32,

    /// Expected improvement in fitness
    pub expected_improvement: f32,

    /// Risk score (0.0 - 1.0, higher = riskier)
    pub risk_score: f32,

    /// Source of gradient (Intuition, User, etc.)
    pub source: GradientSource,
}

/// Source of gradient update
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum GradientSource {
    /// Generated by Intuition Engine
    Intuition,
    /// User feedback
    UserFeedback,
    /// Online learning (real-time)
    OnlineLearning,
    /// Manual adjustment
    Manual,
}

// ============================================================================
// Policy Error
// ============================================================================

/// Errors that can occur during policy operations
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum PolicyError {
    /// Invalid gradient dimensions
    InvalidGradient,
    /// Serialization failed
    SerializationError,
    /// Deserialization failed
    DeserializationError,
    /// Policy update would violate constraints
    ConstraintViolation,
    /// Invalid policy parameters
    InvalidParameters,
}

impl std::fmt::Display for PolicyError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            PolicyError::InvalidGradient => write!(f, "Invalid gradient dimensions"),
            PolicyError::SerializationError => write!(f, "Serialization failed"),
            PolicyError::DeserializationError => write!(f, "Deserialization failed"),
            PolicyError::ConstraintViolation => write!(f, "Policy update would violate constraints"),
            PolicyError::InvalidParameters => write!(f, "Invalid policy parameters"),
        }
    }
}

impl std::error::Error for PolicyError {}

// ============================================================================
// Linear Policy Implementation
// ============================================================================

/// Linear policy: simple weight matrix mapping state → action
///
/// action = W * state + b
///
/// where:
/// - W is 8x8 weight matrix
/// - b is 8-element bias vector
#[derive(Debug, Clone)]
pub struct LinearPolicy {
    /// Weight matrix (8x8 = 64 weights)
    weights: [[f32; 8]; 8],

    /// Bias vector (8 elements)
    bias: [f32; 8],
}

impl LinearPolicy {
    /// Create new linear policy with random initialization
    pub fn new() -> Self {
        Self {
            weights: [[0.0; 8]; 8],
            bias: [0.0; 8],
        }
    }

    /// Create linear policy with Xavier initialization
    ///
    /// Initializes weights from uniform distribution [-limit, limit]
    /// where limit = sqrt(6 / (input_dim + output_dim))
    pub fn with_xavier_init() -> Self {
        let limit = (6.0_f32 / (8.0 + 8.0)).sqrt();
        let mut policy = Self::new();

        // Initialize weights
        for i in 0..8 {
            for j in 0..8 {
                // Simple pseudo-random initialization
                let hash = (i * 31 + j * 17) as f32;
                policy.weights[i][j] = (hash.sin() * 2.0 - 1.0) * limit;
            }
        }

        policy
    }

    /// Get weight at position (i, j)
    pub fn get_weight(&self, i: usize, j: usize) -> f32 {
        self.weights[i][j]
    }

    /// Set weight at position (i, j)
    pub fn set_weight(&mut self, i: usize, j: usize, value: f32) {
        self.weights[i][j] = value;
    }

    /// Get bias at position i
    pub fn get_bias(&self, i: usize) -> f32 {
        self.bias[i]
    }

    /// Set bias at position i
    pub fn set_bias(&mut self, i: usize, value: f32) {
        self.bias[i] = value;
    }
}

impl Default for LinearPolicy {
    fn default() -> Self {
        Self::new()
    }
}

impl Policy for LinearPolicy {
    fn map_state(&self, state: &[f32; 8]) -> [f32; 8] {
        let mut action = [0.0; 8];

        // Matrix multiplication: action = W * state + b
        for i in 0..8 {
            let mut sum = self.bias[i];
            for j in 0..8 {
                sum += self.weights[i][j] * state[j];
            }
            action[i] = sum;
        }

        action
    }

    fn get_gradient(&self, experience: &ExperienceToken) -> Gradient {
        // Simple gradient calculation for linear policy
        // In a real implementation, this would use policy gradient methods
        // For now, we compute a simple delta based on reward

        let mut delta = Vec::with_capacity(64 + 8); // weights + bias

        // Gradient for weights: dW = learning_rate * reward * state * action_error
        for i in 0..8 {
            for j in 0..8 {
                let grad = experience.reward * experience.state[j] * experience.action[i];
                delta.push(grad);
            }
        }

        // Gradient for bias: db = learning_rate * reward * action
        for i in 0..8 {
            let grad = experience.reward * experience.action[i];
            delta.push(grad);
        }

        Gradient {
            delta,
            confidence: 0.5, // Medium confidence for simple linear policy
            expected_improvement: experience.reward.abs() * 0.1,
            risk_score: 0.1, // Low risk for linear updates
            source: GradientSource::OnlineLearning,
        }
    }

    fn apply_gradient(&mut self, gradient: &Gradient, learning_rate: f32) -> Result<(), PolicyError> {
        if gradient.delta.len() != 64 + 8 {
            return Err(PolicyError::InvalidGradient);
        }

        let mut idx = 0;

        // Update weights
        for i in 0..8 {
            for j in 0..8 {
                self.weights[i][j] += learning_rate * gradient.delta[idx];
                idx += 1;
            }
        }

        // Update bias
        for i in 0..8 {
            self.bias[i] += learning_rate * gradient.delta[idx];
            idx += 1;
        }

        Ok(())
    }

    fn serialize(&self) -> Vec<u8> {
        let mut bytes = Vec::with_capacity(64 * 4 + 8 * 4); // 64 weights + 8 bias, 4 bytes each

        // Serialize weights
        for i in 0..8 {
            for j in 0..8 {
                bytes.extend_from_slice(&self.weights[i][j].to_le_bytes());
            }
        }

        // Serialize bias
        for i in 0..8 {
            bytes.extend_from_slice(&self.bias[i].to_le_bytes());
        }

        bytes
    }

    fn deserialize(data: &[u8]) -> Result<Self, PolicyError> {
        if data.len() != (64 + 8) * 4 {
            return Err(PolicyError::DeserializationError);
        }

        let mut policy = Self::new();
        let mut idx = 0;

        // Deserialize weights
        for i in 0..8 {
            for j in 0..8 {
                let bytes: [u8; 4] = data[idx..idx + 4]
                    .try_into()
                    .map_err(|_| PolicyError::DeserializationError)?;
                policy.weights[i][j] = f32::from_le_bytes(bytes);
                idx += 4;
            }
        }

        // Deserialize bias
        for i in 0..8 {
            let bytes: [u8; 4] = data[idx..idx + 4]
                .try_into()
                .map_err(|_| PolicyError::DeserializationError)?;
            policy.bias[i] = f32::from_le_bytes(bytes);
            idx += 4;
        }

        Ok(policy)
    }
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_linear_policy_creation() {
        let policy = LinearPolicy::new();
        assert_eq!(policy.get_weight(0, 0), 0.0);
        assert_eq!(policy.get_bias(0), 0.0);
    }

    #[test]
    fn test_linear_policy_xavier_init() {
        let policy = LinearPolicy::with_xavier_init();
        // Weights should be non-zero after Xavier init
        assert_ne!(policy.get_weight(0, 0), 0.0);
    }

    #[test]
    fn test_linear_policy_map_state() {
        let mut policy = LinearPolicy::new();
        policy.set_weight(0, 0, 1.0);
        policy.set_bias(0, 0.5);

        let state = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0];
        let action = policy.map_state(&state);

        assert_eq!(action[0], 1.5); // 1.0 * 1.0 + 0.5
    }

    #[test]
    fn test_linear_policy_gradient() {
        let policy = LinearPolicy::new();
        let mut exp = ExperienceToken::new(1, 0);
        exp.state = [1.0; 8];
        exp.action = [0.5; 8];
        exp.reward = 10.0;

        let gradient = policy.get_gradient(&exp);
        assert_eq!(gradient.delta.len(), 64 + 8);
        assert_eq!(gradient.source, GradientSource::OnlineLearning);
    }

    #[test]
    fn test_linear_policy_apply_gradient() {
        let mut policy = LinearPolicy::new();
        let mut exp = ExperienceToken::new(1, 0);
        exp.state = [1.0; 8];
        exp.action = [0.5; 8];
        exp.reward = 1.0;

        let gradient = policy.get_gradient(&exp);
        let result = policy.apply_gradient(&gradient, 0.01);
        assert!(result.is_ok());

        // Weights should have changed
        assert_ne!(policy.get_weight(0, 0), 0.0);
    }

    #[test]
    fn test_linear_policy_serialization() {
        let mut policy = LinearPolicy::new();
        policy.set_weight(0, 0, 1.5);
        policy.set_bias(0, 0.5);

        let bytes = policy.serialize();
        assert_eq!(bytes.len(), (64 + 8) * 4);

        let policy2 = LinearPolicy::deserialize(&bytes).unwrap();
        assert_eq!(policy2.get_weight(0, 0), 1.5);
        assert_eq!(policy2.get_bias(0), 0.5);
    }

    #[test]
    fn test_validate_action() {
        let policy = LinearPolicy::new();
        let bounds = [-1.0, 1.0, -1.0, 1.0];

        let valid_action = [0.5; 8];
        assert!(policy.validate_action(&valid_action, &bounds));

        let invalid_action = [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0];
        assert!(!policy.validate_action(&invalid_action, &bounds));
    }
}
